{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2018, 5, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date\n",
    "date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = \"kyubyong. https://github.com/Kyubyong/tensorflow-exercises\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Apply `l2_normalize` to `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05096472 0.10192944 0.15289417 0.20385888 0.2548236  0.30578834\n",
      " 0.35675305 0.40771776 0.45868248 0.5096472 ]\n"
     ]
    }
   ],
   "source": [
    "_x = np.arange(1, 11)\n",
    "epsilon = 1e-12\n",
    "x = tf.convert_to_tensor(_x, tf.float32)\n",
    "\n",
    "output = tf.nn.l2_normalize(x, dim=0, epsilon=epsilon)\n",
    "with tf.Session() as sess:\n",
    "    _output = sess.run(output)\n",
    "\n",
    "    assert np.allclose(_output, _x / np.sqrt(np.maximum(np.sum(_x**2), epsilon)))\n",
    "print(_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Calculate the mean and variance of `x` based on the sufficient statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.5 8.25\n"
     ]
    }
   ],
   "source": [
    "_x = np.arange(1, 11)\n",
    "x = tf.convert_to_tensor(_x, tf.float32)\n",
    "\n",
    "counts_, sum_, sum_of_squares_, _ = tf.nn.sufficient_statistics(x, [0])\n",
    "mean, variance = tf.nn.normalize_moments(counts_, sum_, sum_of_squares_, shift=None)\n",
    "with tf.Session() as sess:\n",
    "    _mean, _variance = sess.run([mean, variance])\n",
    "print(_mean, _variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Calculate the mean and variance of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.5 8.25\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "_x = np.arange(1, 11)\n",
    "x = tf.convert_to_tensor(_x, tf.float32)\n",
    "\n",
    "output = tf.nn.moments(x, [0])\n",
    "with tf.Session() as sess:\n",
    "    _mean, _variance = sess.run(output)\n",
    "print(_mean, _variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Calculate the mean and variance of `x` using `unique_x` and `counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8333334, 0.47222224]\n",
      "[1.8333334, 0.47222227]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.constant([1, 1, 2, 2, 2, 3], tf.float32)\n",
    "\n",
    "# From `x`\n",
    "mean, variance = tf.nn.moments(x, [0])\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([mean, variance]))\n",
    "\n",
    "# From unique elements and their counts\n",
    "unique_x, _, counts = tf.unique_with_counts(x)\n",
    "mean, variance = tf.nn.weighted_moments(unique_x, [0], counts)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([mean, variance]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. The code below is to implement the mnist classification task. Complete it by adding batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-11-dbb34c66e3f0>:21: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "training steps= 100 Acc. = 0.7697\n",
      "training steps= 200 Acc. = 0.8185\n",
      "training steps= 300 Acc. = 0.8532\n",
      "training steps= 400 Acc. = 0.8609\n",
      "training steps= 500 Acc. = 0.8698\n",
      "training steps= 600 Acc. = 0.876\n",
      "training steps= 700 Acc. = 0.882\n",
      "training steps= 800 Acc. = 0.8911\n",
      "training steps= 900 Acc. = 0.8984\n",
      "training steps= 1000 Acc. = 0.8992\n",
      "training steps= 1100 Acc. = 0.8974\n",
      "training steps= 1200 Acc. = 0.9045\n",
      "training steps= 1300 Acc. = 0.9052\n",
      "training steps= 1400 Acc. = 0.9026\n",
      "training steps= 1500 Acc. = 0.9105\n",
      "training steps= 1600 Acc. = 0.9162\n",
      "training steps= 1700 Acc. = 0.9176\n",
      "training steps= 1800 Acc. = 0.9191\n",
      "training steps= 1900 Acc. = 0.9177\n",
      "training steps= 2000 Acc. = 0.9179\n",
      "training steps= 2100 Acc. = 0.9145\n",
      "training steps= 2200 Acc. = 0.9224\n",
      "training steps= 2300 Acc. = 0.9227\n",
      "training steps= 2400 Acc. = 0.9256\n",
      "training steps= 2500 Acc. = 0.9283\n",
      "training steps= 2600 Acc. = 0.9284\n",
      "training steps= 2700 Acc. = 0.9276\n",
      "training steps= 2800 Acc. = 0.927\n",
      "training steps= 2900 Acc. = 0.9293\n",
      "training steps= 3000 Acc. = 0.9243\n",
      "training steps= 3100 Acc. = 0.9253\n",
      "training steps= 3200 Acc. = 0.9327\n",
      "training steps= 3300 Acc. = 0.9274\n",
      "training steps= 3400 Acc. = 0.9325\n",
      "training steps= 3500 Acc. = 0.9352\n",
      "training steps= 3600 Acc. = 0.9342\n",
      "training steps= 3700 Acc. = 0.9343\n",
      "training steps= 3800 Acc. = 0.9346\n",
      "training steps= 3900 Acc. = 0.9385\n",
      "training steps= 4000 Acc. = 0.9339\n",
      "training steps= 4100 Acc. = 0.9397\n",
      "training steps= 4200 Acc. = 0.9345\n",
      "training steps= 4300 Acc. = 0.9413\n",
      "training steps= 4400 Acc. = 0.9389\n",
      "training steps= 4500 Acc. = 0.9399\n",
      "training steps= 4600 Acc. = 0.9409\n",
      "training steps= 4700 Acc. = 0.9388\n",
      "training steps= 4800 Acc. = 0.9402\n",
      "training steps= 4900 Acc. = 0.9388\n",
      "training steps= 5000 Acc. = 0.9429\n",
      "training steps= 5100 Acc. = 0.9438\n",
      "training steps= 5200 Acc. = 0.9429\n",
      "training steps= 5300 Acc. = 0.9401\n",
      "training steps= 5400 Acc. = 0.9428\n",
      "training steps= 5500 Acc. = 0.9407\n",
      "training steps= 5600 Acc. = 0.9385\n",
      "training steps= 5700 Acc. = 0.9408\n",
      "training steps= 5800 Acc. = 0.9433\n",
      "training steps= 5900 Acc. = 0.9436\n",
      "training steps= 6000 Acc. = 0.9461\n",
      "training steps= 6100 Acc. = 0.9474\n",
      "training steps= 6200 Acc. = 0.9466\n",
      "training steps= 6300 Acc. = 0.9437\n",
      "training steps= 6400 Acc. = 0.9461\n",
      "training steps= 6500 Acc. = 0.9489\n",
      "training steps= 6600 Acc. = 0.9446\n",
      "training steps= 6700 Acc. = 0.947\n",
      "training steps= 6800 Acc. = 0.9416\n",
      "training steps= 6900 Acc. = 0.9481\n",
      "training steps= 7000 Acc. = 0.9452\n",
      "training steps= 7100 Acc. = 0.9464\n",
      "training steps= 7200 Acc. = 0.948\n",
      "training steps= 7300 Acc. = 0.9469\n",
      "training steps= 7400 Acc. = 0.9438\n",
      "training steps= 7500 Acc. = 0.9507\n",
      "training steps= 7600 Acc. = 0.9512\n",
      "training steps= 7700 Acc. = 0.9509\n",
      "training steps= 7800 Acc. = 0.9503\n",
      "training steps= 7900 Acc. = 0.9524\n",
      "training steps= 8000 Acc. = 0.9463\n",
      "training steps= 8100 Acc. = 0.9506\n",
      "training steps= 8200 Acc. = 0.9454\n",
      "training steps= 8300 Acc. = 0.9518\n",
      "training steps= 8400 Acc. = 0.9471\n",
      "training steps= 8500 Acc. = 0.9491\n",
      "training steps= 8600 Acc. = 0.9514\n",
      "training steps= 8700 Acc. = 0.9502\n",
      "training steps= 8800 Acc. = 0.9524\n",
      "training steps= 8900 Acc. = 0.9512\n",
      "training steps= 9000 Acc. = 0.9519\n",
      "training steps= 9100 Acc. = 0.9538\n",
      "training steps= 9200 Acc. = 0.9527\n",
      "training steps= 9300 Acc. = 0.9509\n",
      "training steps= 9400 Acc. = 0.9522\n",
      "training steps= 9500 Acc. = 0.9515\n",
      "training steps= 9600 Acc. = 0.9535\n",
      "training steps= 9700 Acc. = 0.9541\n",
      "training steps= 9800 Acc. = 0.9528\n",
      "training steps= 9900 Acc. = 0.9524\n",
      "training steps= 10000 Acc. = 0.9521\n",
      "INFO:tensorflow:Restoring parameters from ./my-model\n",
      "97.0\n"
     ]
    }
   ],
   "source": [
    "# Load data \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=False)\n",
    "\n",
    "# build graph\n",
    "class Graph:\n",
    "    def __init__(self, is_training=False):\n",
    "        # Inputs and labels\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        self.y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "        # Layer 1\n",
    "        w1 = tf.get_variable(\"w1\", shape=[784, 100], initializer=tf.truncated_normal_initializer())\n",
    "        output1 = tf.matmul(self.x, w1)\n",
    "        output1 = tf.contrib.layers.batch_norm(output1, center=True, scale=True, is_training=is_training, \n",
    "                                           updates_collections=None, activation_fn=tf.nn.relu)\n",
    "\n",
    "        #Layer 2\n",
    "        w2 = tf.get_variable(\"w2\", shape=[100, 10], initializer=tf.truncated_normal_initializer())\n",
    "        logits = tf.matmul(output1, w2)\n",
    "        preds = tf.to_int32(tf.arg_max(logits, dimension=1))\n",
    "\n",
    "        # training\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=logits)\n",
    "        self.train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "        self.acc = tf.reduce_mean(tf.to_float(tf.equal(self.y, preds)))\n",
    "\n",
    "\n",
    "# Training\n",
    "tf.reset_default_graph()\n",
    "g = Graph(is_training=True)\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver()\n",
    "    for i in range(1, 10000+1):\n",
    "        batch = mnist.train.next_batch(60)\n",
    "        sess.run(g.train_op, {g.x: batch[0], g.y: batch[1]})\n",
    "        # Evaluation\n",
    "        if i % 100 == 0:\n",
    "            print(\"training steps=\", i, \"Acc. =\", sess.run(g.acc, {g.x: mnist.test.images, g.y: mnist.test.labels}))\n",
    "    save_path = saver.save(sess, './my-model')\n",
    "        \n",
    "# Inference\n",
    "tf.reset_default_graph()\n",
    "g2 = Graph(is_training=False)\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, save_path)\n",
    "    hits = 0\n",
    "    for i in range(100):\n",
    "        hits += sess.run(g2.acc, {g2.x: [mnist.test.images[i]], g2.y: [mnist.test.labels[i]]})\n",
    "    print(hits)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Q06. Compute half the L2 norm of `x` without the sqrt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.5\n",
      "11.5\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.constant([1, 1, 2, 2, 2, 3], tf.float32)\n",
    "\n",
    "output = tf.nn.l2_loss(x)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(output))\n",
    "    print(sess.run(tf.reduce_sum(x**2)/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Compute softmax cross entropy between logits and labels. Note that the rank of them is not the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.7569263 2.1008852 3.680543  3.2403672 2.2937908]\n",
      " [0.8207513 2.5667608 3.325022  2.5893521 1.3432555]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "logits = tf.random_normal(shape=[2, 5, 10])\n",
    "labels = tf.convert_to_tensor(np.random.randint(0, 10, size=[2, 5]), tf.int32)\n",
    "output = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Compute softmax cross entropy between logits and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-2b667fec9158>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "[[3.4845574 2.4588187 3.415313  0.8436859 2.7065454]\n",
      " [2.4679606 2.3892205 3.926594  1.2501802 3.1182024]]\n"
     ]
    }
   ],
   "source": [
    "logits = tf.random_normal(shape=[2, 5, 10])\n",
    "labels = tf.convert_to_tensor(np.random.randint(0, 10, size=[2, 5]), tf.int32)\n",
    "labels = tf.one_hot(labels, depth=10)\n",
    "\n",
    "output = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Map tensor `x` to the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.2 0.1 0.3 0.4]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.constant([0, 2, 1, 3, 4], tf.int32)\n",
    "embedding = tf.constant([0, 0.1, 0.2, 0.3, 0.4], tf.float32)\n",
    "output = tf.nn.embedding_lookup(embedding, x)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(output))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
